"""
H-GFA Net è®­ç»ƒä»£ç  - æœ€å°æ”¹åŠ¨ç‰ˆ
ä¿ç•™visual_featuresçš„1280ç»´åº¦ï¼Œä½¿ç”¨æ­£ç¡®çš„æ•°æ®åº“ç»“æ„
å……åˆ†åˆ©ç”¨MPSåŠ é€Ÿ
"""

import sqlite3
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import matplotlib.pyplot as plt

# å¯¼å…¥Stageæ¨¡å—
from stage1_cdcaf import CDCAF
from stage2_gqca import GQCA
from stage3_mfa import MFA


# =========================
# é…ç½®å‚æ•°
# =========================
DB_PATH = 'facialPalsy.db'
BATCH_SIZE = 16
NUM_EPOCHS = 50
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-4

# ä½¿ç”¨MPSåŠ é€Ÿï¼ˆMacBook Proï¼‰
if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
    print("ğŸš€ ä½¿ç”¨ Apple Silicon MPS åŠ é€Ÿ")
    # MPSä¼˜åŒ–è®¾ç½®
    torch.mps.set_per_process_memory_fraction(0.0)  # è‡ªåŠ¨ç®¡ç†å†…å­˜
else:
    DEVICE = torch.device("cpu")

# åŠ¨ä½œç»´åº¦é…ç½®ï¼ˆä»æ•°æ®ç»Ÿè®¡å¾—å‡ºï¼‰
ACTION_DIMS = {
    'BlowCheek': (5, 2),
    'CloseEyeHardly': (10, 8),
    'CloseEyeSoftly': (7, 4),
    'LipPucker': (5, 2),
    'NeutralFace': (8, 0),
    'RaiseEyebrow': (10, 3),
    'ShowTeeth': (7, 3),
    'ShrugNose': (5, 2),
    'Smile': (11, 4),
    'SpontaneousEyeBlink': (5, 7),
    'VoluntaryEyeBlink': (5, 4),
}


# =========================
# 1. æ•°æ®é›†ç±»
# =========================
class FacialPalsyDataset(Dataset):
    """é¢ç˜«è¯„ä¼°æ•°æ®é›† - æ”¯æŒåŠ¨ä½œåˆ†ç»„å’Œå¯å˜ç»´åº¦"""

    def __init__(self, video_ids, data_dict, labels, action_names):
        self.video_ids = video_ids
        self.data_dict = data_dict
        self.labels = labels
        self.action_names = action_names

        # æŒ‰åŠ¨ä½œåˆ†ç»„ï¼Œä¾¿äºæ‰¹å¤„ç†
        self.action_groups = self._group_by_action()

    def _group_by_action(self):
        """æŒ‰åŠ¨ä½œç±»å‹åˆ†ç»„"""
        groups = defaultdict(list)
        for idx, vid in enumerate(self.video_ids):
            action = self.action_names[vid]
            groups[action].append(idx)
        return groups

    def __len__(self):
        return len(self.video_ids)

    def __getitem__(self, idx):
        vid = self.video_ids[idx]

        # è·å–ç‰¹å¾ï¼ˆæ³¨æ„å¤„ç†å¯èƒ½çš„Noneå€¼ï¼‰
        static_feat = self.data_dict['static_features'][idx]
        dynamic_feat = self.data_dict['dynamic_features'][idx]
        visual_feat = self.data_dict['visual_features'][idx]

        # è½¬æ¢ä¸ºtensorï¼Œç¡®ä¿åœ¨MPSè®¾å¤‡ä¸Šçš„æ•°æ®ç±»å‹æ­£ç¡®
        static_tensor = torch.tensor(static_feat, dtype=torch.float32).contiguous()
        dynamic_tensor = torch.tensor(dynamic_feat, dtype=torch.float32).contiguous() if dynamic_feat is not None and len(dynamic_feat) > 0 else None
        visual_tensor = torch.tensor(visual_feat, dtype=torch.float32).contiguous()

        return {
            'video_id': vid,
            'action': self.action_names[vid],
            'static': static_tensor,
            'dynamic': dynamic_tensor,
            'visual': visual_tensor,
            'label': self.labels[vid] - 1  # è½¬ä¸º0-4ï¼ˆåŸå§‹æ˜¯1-5ï¼‰
        }


# =========================
# 2. è‡ªå®šä¹‰collateå‡½æ•°ï¼ˆå…³é”®ï¼ï¼‰
# =========================
def collate_fn_padding(batch):
    """
    ä½¿ç”¨ padding å¤„ç†ä¸åŒç»´åº¦çš„ç‰¹å¾
    æŒ‰åŠ¨ä½œåˆ†ç»„ï¼›å¯¹æ¯ä¸ªåŠ¨ä½œå†…éƒ¨çš„æ ·æœ¬åˆ†åˆ«åš padding
    """
    from collections import defaultdict
    import torch

    # 1. æŒ‰åŠ¨ä½œåˆ†ç»„
    action_groups = defaultdict(list)
    for item in batch:
        action_groups[item['action']].append(item)

    batch_dict = {}

    for action, items in action_groups.items():
        if not items:
            continue

        # ---------- é™æ€ç‰¹å¾ç»´åº¦ ----------
        max_static_dim = max(item['static'].shape[0] for item in items)

        # ---------- åŠ¨æ€ç‰¹å¾ç»´åº¦ï¼ˆå¯èƒ½å…¨æ˜¯ Noneï¼‰ ----------
        dyn_dims = [
            item['dynamic'].shape[0]
            for item in items
            if (item['dynamic'] is not None and item['dynamic'].numel() > 0)
        ]
        if len(dyn_dims) > 0:
            max_dynamic_dim = max(dyn_dims)
        else:
            # è¿™ä¸€ç»„åŠ¨ä½œï¼ˆæ¯”å¦‚ NeutralFaceï¼‰å®Œå…¨æ²¡æœ‰åŠ¨æ€ç‰¹å¾
            max_dynamic_dim = 0

        padded_static = []
        padded_dynamic = []
        visual_list = []
        label_list = []

        for item in items:
            # ---------- é™æ€ç‰¹å¾ padding ----------
            static = item['static']
            if static.shape[0] < max_static_dim:
                pad = torch.zeros(max_static_dim - static.shape[0], dtype=static.dtype)
                static = torch.cat([static, pad])
            padded_static.append(static)

            # ---------- åŠ¨æ€ç‰¹å¾ padding ----------
            if max_dynamic_dim > 0:
                dyn = item['dynamic']
                if dyn is None or dyn.numel() == 0:
                    # æ²¡æœ‰åŠ¨æ€ç‰¹å¾ï¼Œç”¨å…¨ 0 å¡«å……
                    dyn = torch.zeros(max_dynamic_dim, dtype=static.dtype)
                elif dyn.shape[0] < max_dynamic_dim:
                    pad = torch.zeros(max_dynamic_dim - dyn.shape[0], dtype=dyn.dtype)
                    dyn = torch.cat([dyn, pad])
                padded_dynamic.append(dyn)

            # ---------- è§†è§‰ç‰¹å¾ & æ ‡ç­¾ ----------
            visual_list.append(item['visual'])
            label_list.append(item['label'])

        batch_dict[action] = {
            'static': torch.stack(padded_static),
            'dynamic': torch.stack(padded_dynamic) if max_dynamic_dim > 0 else None,
            'visual': torch.stack(visual_list),
            'labels': torch.tensor(label_list, dtype=torch.long),
            'static_dim': max_static_dim,
            'dynamic_dim': max_dynamic_dim,
        }

    return batch_dict



# =========================
# 3. H-GFA Netæ¨¡å‹ï¼ˆæœ€å°æ”¹åŠ¨ç‰ˆï¼‰
# =========================
class HGFANet(nn.Module):
    """
    Hierarchical Geometry-Visual Fusion Attention Network
    æœ€å°æ”¹åŠ¨ï¼šä¸ºæ¯ä¸ªåŠ¨ä½œåˆ›å»ºç‹¬ç«‹çš„Stage1ç¼–ç å™¨
    ä¿ç•™visual_featuresçš„1280ç»´
    """

    def __init__(self, action_dims=ACTION_DIMS, num_classes=5, device='cpu'):
        super().__init__()
        self.action_dims = action_dims
        self.num_classes = num_classes  # 5ä¸ªä¸¥é‡ç¨‹åº¦ç­‰çº§
        self.device = device

        # ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ›å»ºç‹¬ç«‹çš„Stage1ç¼–ç å™¨ï¼ˆå¤„ç†å¯å˜ç»´åº¦ï¼‰
        self.stage1_modules = nn.ModuleDict()

        # æ‰¾å‡ºæœ€å¤§ç»´åº¦ï¼ˆç”¨äºåˆ›å»ºç»Ÿä¸€çš„ç¼–ç å™¨ï¼‰
        max_static_dim = max(d[0] for d in action_dims.values())
        max_dynamic_dim = max(d[1] for d in action_dims.values())

        for action, (s_dim, d_dim) in action_dims.items():
            # æ¯ä¸ªåŠ¨ä½œç”¨è‡ªå·±çœŸå®çš„é™æ€/åŠ¨æ€ç‰¹å¾ç»´åº¦
            self.stage1_modules[action] = CDCAF(
                static_dim=s_dim,
                dynamic_dim=d_dim,
                clinical_dim=0,
                d_model=128,
                num_layers=2,
                num_heads=4,
                output_dim=256
            )

        # Stage2: å‡ ä½•å¼•å¯¼è§†è§‰æ³¨æ„åŠ›ï¼ˆå…±äº«ï¼‰
        self.stage2 = GQCA(
            geo_dim=256,
            visual_dim=1280,  # ä¿æŒåŸå§‹ç»´åº¦
            d_model=256,
            num_heads=8,
            num_layers=2,
            num_tokens=49,
            out_dim=256
        )

        # Stage3: å¤šæ¨¡æ€èåˆï¼ˆå…±äº«ï¼‰
        self.stage3 = MFA(
            geo_dim=256,
            visual_guided_dim=256,
            visual_global_dim=1280,  # ä¿æŒåŸå§‹ç»´åº¦
            feature_dim=256,
            num_heads=4,
            num_layers=2,
            output_dim=512
        )

        # åˆ†ç±»å¤´ï¼ˆ5ä¸ªä¸¥é‡ç¨‹åº¦ç­‰çº§ï¼‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(512),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.to(device)

    def forward(self, batch_dict):
        """
        å‰å‘ä¼ æ’­
        batch_dict: æŒ‰åŠ¨ä½œåˆ†ç»„çš„æ‰¹æ¬¡æ•°æ®
        """
        all_logits = []
        all_labels = []

        for action, data in batch_dict.items():
            # ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆMPSä¼˜åŒ–ï¼‰
            static = data['static'].to(self.device, non_blocking=True)
            dynamic = data['dynamic'].to(self.device, non_blocking=True) if data['dynamic'] is not None else None
            visual = data['visual'].to(self.device, non_blocking=True)
            labels = data['labels'].to(self.device, non_blocking=True)

            # Stage1: å‡ ä½•ç‰¹å¾èåˆï¼ˆåŠ¨ä½œç‰¹å¼‚æ€§ï¼‰
            stage1_module = self.stage1_modules[action]
            geo_refined = stage1_module(static, dynamic)  # (B, 256)

            # Stage2: å‡ ä½•å¼•å¯¼è§†è§‰æ³¨æ„åŠ›
            visual_guided = self.stage2(geo_refined, visual)  # (B, 256)

            # Stage3: å¤šæ¨¡æ€èåˆ
            fused = self.stage3(geo_refined, visual_guided, visual)  # (B, 512)

            # åˆ†ç±»
            logits = self.classifier(fused)  # (B, 5)

            all_logits.append(logits)
            all_labels.append(labels)

        # åˆå¹¶æ‰€æœ‰åŠ¨ä½œçš„ç»“æœ
        if all_logits:
            combined_logits = torch.cat(all_logits, dim=0)
            combined_labels = torch.cat(all_labels, dim=0)
        else:
            # å¤„ç†ç©ºæ‰¹æ¬¡
            combined_logits = torch.empty(0, self.num_classes, device=self.device)
            combined_labels = torch.empty(0, dtype=torch.long, device=self.device)

        return combined_logits, combined_labels


# =========================
# 4. æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ•°æ®åº“è¡¨ï¼‰
# =========================
def load_data_from_db(db_path):
    """ä»æ•°æ®åº“åŠ è½½è®­ç»ƒæ•°æ®"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # æ­£ç¡®çš„æŸ¥è¯¢ï¼šä»video_featureså’Œaction_labelsè¡¨è·å–æ•°æ®
    query = """
    SELECT 
        vf.video_id,
        at.action_name_en,
        vfeat.static_features,
        vfeat.dynamic_features,
        vfeat.visual_features,
        vfeat.static_dim,
        vfeat.dynamic_dim,
        al.severity_score
    FROM video_files vf
    INNER JOIN video_features vfeat ON vf.video_id = vfeat.video_id
    INNER JOIN action_types at ON vf.action_id = at.action_id
    INNER JOIN action_labels al ON vf.examination_id = al.examination_id 
        AND vf.action_id = al.action_id
    WHERE vfeat.static_features IS NOT NULL
      AND vfeat.visual_features IS NOT NULL
      AND al.severity_score BETWEEN 1 AND 5
    """

    cursor.execute(query)
    rows = cursor.fetchall()
    conn.close()

    print(f"\n[DataLoader] ä»æ•°æ®åº“åŠ è½½ {len(rows)} ä¸ªæ ·æœ¬")

    # è§£ææ•°æ®
    video_ids = []
    action_names = {}
    static_features = []
    dynamic_features = []
    visual_features = []
    labels = {}

    # åŠ¨ä½œåˆ†å¸ƒç»Ÿè®¡
    action_counts = defaultdict(int)
    dim_stats = defaultdict(list)

    for row in rows:
        vid, action, static_blob, dynamic_blob, visual_blob, s_dim, d_dim, severity = row

        # è·³è¿‡æ— æ•ˆæ•°æ®
        if not static_blob or not visual_blob:
            continue

        # è§£æç‰¹å¾ï¼ˆä»BLOBï¼‰
        static_feat = np.frombuffer(static_blob, dtype=np.float32)
        dynamic_feat = np.frombuffer(dynamic_blob, dtype=np.float32) if dynamic_blob else np.zeros(0, dtype=np.float32)
        visual_feat = np.frombuffer(visual_blob, dtype=np.float32)

        # éªŒè¯ç»´åº¦
        if visual_feat.shape[0] != 1280:
            print(f"è­¦å‘Šï¼šè§†è§‰ç‰¹å¾ç»´åº¦ä¸æ­£ç¡® {vid}: {visual_feat.shape}")
            continue

        video_ids.append(vid)
        action_names[vid] = action
        static_features.append(static_feat)
        dynamic_features.append(dynamic_feat if len(dynamic_feat) > 0 else None)
        visual_features.append(visual_feat)
        labels[vid] = severity

        action_counts[action] += 1
        dim_stats[action].append((s_dim, d_dim))

    print(f"[DataLoader] æœ‰æ•ˆæ ·æœ¬æ•°: {len(video_ids)}")
    print("[DataLoader] åŠ¨ä½œåˆ†å¸ƒ:")
    for action in sorted(action_counts.keys()):
        dims = dim_stats[action]
        if dims:
            s_dims = [d[0] for d in dims]
            d_dims = [d[1] for d in dims]
            print(f"  {action:25} {action_counts[action]:3} ä¸ªæ ·æœ¬, "
                  f"é™æ€ç»´åº¦={min(s_dims)}-{max(s_dims)}, "
                  f"åŠ¨æ€ç»´åº¦={min(d_dims)}-{max(d_dims)}")

    # æ„å»ºæ•°æ®å­—å…¸
    data_dict = {
        'static_features': static_features,
        'dynamic_features': dynamic_features,
        'visual_features': visual_features
    }

    return data_dict, labels, action_names


# =========================
# 5. è®­ç»ƒå™¨ç±»ï¼ˆMPSä¼˜åŒ–ï¼‰
# =========================
class Trainer:
    """H-GFA Netè®­ç»ƒå™¨ - MPSä¼˜åŒ–ç‰ˆ"""

    def __init__(self, model, train_loader, val_loader, device,
                 learning_rate=1e-4, weight_decay=1e-4, save_dir='checkpoints'):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # ä¼˜åŒ–å™¨ï¼ˆMPSä¼˜åŒ–ï¼‰
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            fused=True if device.type == 'cuda' else False  # CUDAæ”¯æŒfusedä¼˜åŒ–
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()

        # ä¿å­˜è·¯å¾„
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)

        # è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        self.val_accs = []
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0.0
        num_samples = 0
        correct = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch} [Train]")

        for batch_idx, batch_dict in enumerate(pbar):
            if not batch_dict:
                continue

            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad(set_to_none=True)  # MPSä¼˜åŒ–ï¼šset_to_noneæ›´é«˜æ•ˆ

            # å‰å‘ä¼ æ’­
            logits, labels = self.model(batch_dict)

            if logits.numel() == 0:
                continue

            # è®¡ç®—æŸå¤±
            loss = self.criterion(logits, labels)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            # æ›´æ–°å‚æ•°
            self.optimizer.step()

            # ç»Ÿè®¡
            batch_size = labels.size(0)
            epoch_loss += loss.item() * batch_size
            num_samples += batch_size

            # è®¡ç®—å‡†ç¡®ç‡
            _, preds = torch.max(logits, 1)
            correct += (preds == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            if num_samples > 0:
                avg_loss = epoch_loss / num_samples
                acc = correct / num_samples
                pbar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'acc': f'{acc:.4f}',
                    'batch': f'{batch_idx+1}/{len(self.train_loader)}'
                })

            # MPSå†…å­˜ç®¡ç†
            if self.device.type == 'mps' and batch_idx % 10 == 0:
                torch.mps.synchronize()  # å®šæœŸåŒæ­¥ï¼Œé˜²æ­¢å†…å­˜ç´¯ç§¯

        return epoch_loss / num_samples if num_samples > 0 else 0.0

    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        val_loss = 0.0
        num_samples = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"Epoch {epoch} [Val]")

            for batch_dict in pbar:
                if not batch_dict:
                    continue

                # å‰å‘ä¼ æ’­
                logits, labels = self.model(batch_dict)

                if logits.numel() == 0:
                    continue

                # è®¡ç®—æŸå¤±
                loss = self.criterion(logits, labels)

                # ç»Ÿè®¡
                batch_size = labels.size(0)
                val_loss += loss.item() * batch_size
                num_samples += batch_size

                # é¢„æµ‹
                _, preds = torch.max(logits, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # æ›´æ–°è¿›åº¦æ¡
                if num_samples > 0:
                    pbar.set_postfix({'loss': f'{val_loss/num_samples:.4f}'})

        if num_samples > 0:
            avg_loss = val_loss / num_samples
            accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)

            self.val_losses.append(avg_loss)
            self.val_accs.append(accuracy)

            # æ›´æ–°æœ€ä½³æ¨¡å‹
            if avg_loss < self.best_val_loss:
                self.best_val_loss = avg_loss
                self.best_val_acc = accuracy
                self.save_checkpoint(epoch, 'best_model.pth')

            return avg_loss, accuracy, all_preds, all_labels
        else:
            return 0.0, 0.0, [], []

    def save_checkpoint(self, epoch, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accs': self.val_accs,
            'best_val_loss': self.best_val_loss,
            'best_val_acc': self.best_val_acc,
            'action_dims': self.model.action_dims,
            'device': str(self.device)
        }

        save_path = self.save_dir / filename
        torch.save(checkpoint, save_path)
        print(f"\n[Checkpoint] æ¨¡å‹å·²ä¿å­˜: {save_path}")

    def plot_curves(self):
        """ ç”»å‡ºè®­ç»ƒ / éªŒè¯æ›²çº¿å¹¶ä¿å­˜ä¸º PNG"""
        if len(self.train_losses) == 0 or len(self.val_losses) == 0:
            print("âš  æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒè®°å½•ï¼Œæ— æ³•ç”»æ›²çº¿")
            return

        epochs = range(1, len(self.train_losses) + 1)

        # 1) æŸå¤±æ›²çº¿
        plt.figure()
        plt.plot(epochs, self.train_losses, marker='o', label='Train Loss')
        plt.plot(epochs, self.val_losses, marker='o', label='Val Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Train & Val Loss')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        loss_path = self.save_dir / 'loss_curve.png'
        plt.savefig(loss_path, dpi=300)
        plt.close()
        print(f"ğŸ“‰ æŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_path}")

        # 2) éªŒè¯å‡†ç¡®ç‡æ›²çº¿
        if len(self.val_accs) > 0:
            plt.figure()
            plt.plot(epochs, self.val_accs, marker='o', label='Val Accuracy')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.title('Val Accuracy')
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            acc_path = self.save_dir / 'val_acc_curve.png'
            plt.savefig(acc_path, dpi=300)
            plt.close()
            print(f" éªŒè¯å‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜: {acc_path}")

    def train(self, num_epochs):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        print("\n" + "="*60)
        print(" å¼€å§‹è®­ç»ƒ H-GFA Net")
        print(f" è®¾å¤‡: {self.device}")
        print("="*60)

        for epoch in range(1, num_epochs + 1):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch}/{num_epochs}")
            print(f"{'='*60}")

            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            # éªŒè¯
            val_loss, val_acc, preds, labels = self.validate(epoch)

            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_loss)

            # æ‰“å°ç»“æœ
            print(f"\n[Epoch {epoch}] ç»“æœ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

            # æ¯5ä¸ªepochæ‰“å°è¯¦ç»†æŠ¥å‘Š
            if epoch % 5 == 0 and len(labels) > 0:
                print("\nåˆ†ç±»æŠ¥å‘Šï¼ˆä¸¥é‡ç¨‹åº¦1-5çº§ï¼‰:")
                print(classification_report(
                    labels, preds,
                    target_names=[f"Grade {i+1}" for i in range(5)],
                    zero_division=0
                ))

            # æ¯10ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                self.save_checkpoint(epoch, f'checkpoint_epoch_{epoch}.pth')

            # MPSå†…å­˜æ¸…ç†
            if self.device.type == 'mps':
                torch.mps.empty_cache()

        print("\n" + "="*60)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print("="*60)
        self.plot_curves()


# =========================
# 6. ä¸»å‡½æ•°
# =========================
def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("="*60)
    print("H-GFA Net è®­ç»ƒ - MacBook Pro MPSä¼˜åŒ–ç‰ˆ")
    print("="*60)
    print(f"ğŸ“± è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“¦ æ‰¹å¤§å°: {BATCH_SIZE}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"ğŸ¯ ä»»åŠ¡: é¢ç˜«ä¸¥é‡ç¨‹åº¦åˆ†çº§ï¼ˆ1-5çº§ï¼‰")

    # 1. åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    data_dict, labels, action_names = load_data_from_db(DB_PATH)

    if not labels:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        print("è¯·æ£€æŸ¥æ•°æ®åº“æ˜¯å¦åŒ…å«å¿…è¦çš„æ•°æ®")
        return

    # 2. åˆ’åˆ†æ•°æ®é›†
    print("\n[2/5] åˆ’åˆ†æ•°æ®é›†...")
    all_video_ids = list(labels.keys())

    # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†å±‚æŠ½æ ·
    train_ids, val_ids = train_test_split(
        all_video_ids,
        test_size=0.2,
        random_state=42,
        stratify=[labels[vid] for vid in all_video_ids]
    )

    print(f"  è®­ç»ƒé›†: {len(train_ids)} ä¸ªæ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_ids)} ä¸ªæ ·æœ¬")

    # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
    train_severity = [labels[vid] for vid in train_ids]
    val_severity = [labels[vid] for vid in val_ids]
    print("\nä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    for i in range(1, 6):
        train_count = train_severity.count(i)
        val_count = val_severity.count(i)
        print(f"  Grade {i}: è®­ç»ƒ={train_count}, éªŒè¯={val_count}")

    # 3. åˆ›å»ºæ•°æ®é›†
    print("\n[3/5] åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    # åˆ†å‰²æ•°æ®
    def get_indices(video_ids, all_ids):
        return [all_ids.index(vid) for vid in video_ids]

    train_indices = get_indices(train_ids, all_video_ids)
    val_indices = get_indices(val_ids, all_video_ids)

    train_data_dict = {
        'static_features': [data_dict['static_features'][i] for i in train_indices],
        'dynamic_features': [data_dict['dynamic_features'][i] for i in train_indices],
        'visual_features': [data_dict['visual_features'][i] for i in train_indices]
    }

    val_data_dict = {
        'static_features': [data_dict['static_features'][i] for i in val_indices],
        'dynamic_features': [data_dict['dynamic_features'][i] for i in val_indices],
        'visual_features': [data_dict['visual_features'][i] for i in val_indices]
    }

    train_dataset = FacialPalsyDataset(train_ids, train_data_dict, labels, action_names)
    val_dataset = FacialPalsyDataset(val_ids, val_data_dict, labels, action_names)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆMPSä¼˜åŒ–ï¼špin_memoryå¯¹MPSæ— æ•ˆï¼Œä½¿ç”¨non_blockingï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn_padding,
        num_workers=0,  # MPSä¸æ”¯æŒå¤šè¿›ç¨‹
        persistent_workers=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_fn_padding,
        num_workers=0,
        persistent_workers=False
    )

    # 4. ç»Ÿè®¡åŠ¨ä½œç»´åº¦
    print("\n[4/5] ç»Ÿè®¡åŠ¨ä½œç»´åº¦...")
    print(f"å…± {len(ACTION_DIMS)} ä¸ªåŠ¨ä½œ:")
    for action, (s_dim, d_dim) in ACTION_DIMS.items():
        print(f"  {action:25} é™æ€={s_dim:<3} åŠ¨æ€={d_dim:<3}")

    # 5. åˆ›å»ºæ¨¡å‹
    print("\n[5/5] åˆ›å»ºæ¨¡å‹...")
    model = HGFANet(
        action_dims=ACTION_DIMS,
        num_classes=5,  # 5ä¸ªä¸¥é‡ç¨‹åº¦ç­‰çº§
        device=DEVICE
    )

    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"ğŸ’¾ æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f} MB")

    # 6. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=DEVICE,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        save_dir='checkpoints'
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_epochs=NUM_EPOCHS)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ° checkpoints/ ç›®å½•")
    print(f"ğŸ¯ æœ€ä½³æ¨¡å‹: checkpoints/best_model.pth")

if __name__ == "__main__":
    main()