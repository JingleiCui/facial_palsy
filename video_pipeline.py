"""
è§†é¢‘å¤„ç†Pipeline V3 - å½»åº•ä¿®å¤å†…å­˜é—®é¢˜
ä¸»è¦æ”¹è¿›:
1. é™ä½Žå¹¶è¡Œåº¦é¿å…MediaPipeå†²çª
2. åŠæ—¶é‡Šæ”¾å¸§å†…å­˜
3. å®šæœŸåžƒåœ¾å›žæ”¶
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

import cv2
import numpy as np
import sqlite3
import json
import gc
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

from facialPalsy.core.landmark_extractor import LandmarkExtractor
from facialPalsy.action_feature_integrator import ActionFeatureIntegrator


class VideoPipeline:
    """
    è§†é¢‘å¤„ç†Pipeline

    V3æ”¹è¿›:
    1. é™ä½Žå¹¶è¡Œåº¦(3çº¿ç¨‹)é¿å…MediaPipe GPUå†²çª
    2. åŠæ—¶é‡Šæ”¾å¸§åºåˆ—å†…å­˜
    3. å®šæœŸå¼ºåˆ¶åžƒåœ¾å›žæ”¶
    4. åˆ†æ‰¹å¤„ç†examinations
    """

    def __init__(self, db_path, model_path, keyframe_root_dir):
        """
        Args:
            db_path: æ•°æ®åº“è·¯å¾„
            keyframe_root_dir: å…³é”®å¸§ä¿å­˜æ ¹ç›®å½•
        """
        self.db_path = db_path
        self.keyframe_root_dir = Path(keyframe_root_dir)
        self.keyframe_root_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–landmarkæå–å™¨
        self.landmark_extractor = LandmarkExtractor(model_path)

        # åˆå§‹åŒ–ç‰¹å¾æ•´åˆå™¨
        self.feature_integrator = ActionFeatureIntegrator()

        # åˆå§‹åŒ–åŠ¨ä½œæ£€æµ‹å™¨
        self.action_detectors = self.feature_integrator.action_detectors

        # é™æ¯å¸§ç¼“å­˜
        self.neutral_cache = {}

        self.model_path = model_path

        # ðŸ”§ å…³é”®ä¿®å¤1: é™ä½Žå¹¶è¡Œåº¦é¿å…MediaPipe GPUå†²çª
        # MediaPipeåœ¨å¤šçº¿ç¨‹ä¸­ä¼šåˆ›å»ºå¤šä¸ªOpenGLä¸Šä¸‹æ–‡,å®¹æ˜“OOM
        self.num_workers = 6  # æ¯ä¸ªçº¿ç¨‹çº¦500MBæ¨¡åž‹

        self._tls = threading.local()

    def _get_worker(self):
        w = getattr(self._tls, "worker", None)
        if w is None:
            # æ¯ä¸ªçº¿ç¨‹å„è‡ªæŒæœ‰ä¸€å¥—æ¨¡åž‹/æ£€æµ‹å™¨
            w = type("Worker", (), {})()
            w.landmark_extractor = LandmarkExtractor(self.model_path)
            w.feature_integrator = ActionFeatureIntegrator()
            w.action_detectors = w.feature_integrator.action_detectors
            self._tls.worker = w
        return w

    def process_examination(self, examination_id):
        """
        å¤„ç†ä¸€ä¸ªå®Œæ•´çš„examination(11ä¸ªåŠ¨ä½œ)

        Args:
            examination_id: æ£€æŸ¥ID

        Returns:
            dict: å¤„ç†ç»“æžœ
        """
        print(f"\n{'=' * 60}")
        print(f"å¤„ç†æ£€æŸ¥ ID: {examination_id}")
        print(f"{'=' * 60}")

        start_time = datetime.now()

        # 1. èŽ·å–è¯¥examinationçš„æ‰€æœ‰è§†é¢‘
        videos = self._get_examination_videos(examination_id)

        if not videos:
            print(f"[ERROR] æ£€æŸ¥ {examination_id} æ²¡æœ‰è§†é¢‘")
            return None

        print(f"æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")

        # 2. é¦–å…ˆå¤„ç†NeutralFace(é™æ¯å¸§)
        neutral_result = None
        neutral_video = next((v for v in videos if v['action_name_en'] == 'NeutralFace'), None)

        if neutral_video:
            print("\n[æ­¥éª¤1] å¤„ç†é™æ¯å¸§...")
            neutral_result = self.process_video(
                neutral_video['video_id'],
                neutral_indicators=None
            )

            if neutral_result:
                # ç¼“å­˜é™æ¯å¸§æŒ‡æ ‡
                self.neutral_cache[examination_id] = {
                    'normalized_indicators': neutral_result['normalized_indicators'],
                    'peak_frame_idx': neutral_result['peak_frame_idx']
                }
                print(f"âœ“ é™æ¯å¸§å¤„ç†å®Œæˆ")

        # 3. å¤„ç†å…¶ä»–10ä¸ªåŠ¨ä½œ
        results = {}
        other_videos = [v for v in videos if v['action_name_en'] != 'NeutralFace']

        print(f"\n[æ­¥éª¤2] å¹¶è¡Œå¤„ç†å…¶ä½™ {len(other_videos)} ä¸ªåŠ¨ä½œ...")

        neutral_indicators = None
        if examination_id in self.neutral_cache:
            neutral_indicators = self.neutral_cache[examination_id]['normalized_indicators']

        failures = []
        computed = []

        with ThreadPoolExecutor(max_workers=self.num_workers) as ex:
            fut_map = {ex.submit(self._compute_video_only, v, neutral_indicators): v for v in other_videos}
            for fut in as_completed(fut_map):
                v = fut_map[fut]
                try:
                    out = fut.result()
                except Exception as e:
                    failures.append((v['video_id'], v['action_name_en'], str(e)))
                    continue
                if not out.get("ok"):
                    failures.append((v['video_id'], v['action_name_en'], out.get("error", "unknown")))
                    continue
                computed.append(out)

        # ä¸²è¡Œä¿å­˜(é¿å… SQLite å†™é”)
        for out in computed:
            vinfo = next(v for v in other_videos if v["video_id"] == out["video_id"])
            action_name = out["action_name"]
            r = out["result"]

            peak_frame_path = self._save_peak_frame(r['peak_frame'], vinfo['examination_id'], action_name)

            # ðŸ”§ å…³é”®ä¿®å¤2: ç«‹å³é‡Šæ”¾å³°å€¼å¸§
            del r['peak_frame']

            self._save_to_database(
                video_id=vinfo['video_id'],
                peak_frame_idx=r['peak_frame_idx'],
                peak_frame_path=str(peak_frame_path),
                unit_length=r['unit_length'],
                feature_vector=out["feature_vector"],
                normalized_indicators=r['normalized_indicators'],
                normalized_dynamic_features=r['normalized_dynamic_features']
            )

            results[action_name] = {
                'video_id': vinfo['video_id'],
                'action_name': action_name,
                'peak_frame_idx': r['peak_frame_idx'],
                'peak_frame_path': str(peak_frame_path),
                'unit_length': r['unit_length'],
                'feature_dim': out["feature_vector"].shape[0],
                'feature_vector': out["feature_vector"],
                'normalized_indicators': r['normalized_indicators'],
                'normalized_dynamic_features': r['normalized_dynamic_features']
            }

            # ðŸ”§ å…³é”®ä¿®å¤3: é‡Šæ”¾computedä¸­çš„å¤§å¯¹è±¡
            del out["result"]

        if failures:
            print(f"  [WARN] æœ¬æ¬¡ examination æœ‰ {len(failures)} ä¸ªåŠ¨ä½œå¤±è´¥ï¼š")
            for vid, act, err in failures[:10]:
                print(f"    - video_id={vid} act={act} err={err}")

        # 4. æ·»åŠ é™æ¯å¸§ç»“æžœ
        if neutral_result:
            results['NeutralFace'] = neutral_result

        processing_time = (datetime.now() - start_time).total_seconds() * 1000

        print(f"\n{'=' * 60}")
        print(f"âœ“ æ£€æŸ¥å¤„ç†å®Œæˆ! è€—æ—¶: {processing_time:.2f}ms")
        print(f"æˆåŠŸå¤„ç†: {len(results)}/11 ä¸ªåŠ¨ä½œ")
        print(f"{'=' * 60}")

        # ðŸ”§ å…³é”®ä¿®å¤4: å¼ºåˆ¶åžƒåœ¾å›žæ”¶
        del computed
        gc.collect()

        return {
            'examination_id': examination_id,
            'results': results,
            'processing_time_ms': processing_time
        }

    def process_video(self, video_id, neutral_indicators=None):
        """
        å¤„ç†å•ä¸ªè§†é¢‘

        Args:
            video_id: è§†é¢‘ID
            neutral_indicators: é™æ¯å¸§çš„å½’ä¸€åŒ–æŒ‡æ ‡(ç”¨äºŽå¯¹æ¯”)

        Returns:
            dict: å¤„ç†ç»“æžœ
        """
        # 1. èŽ·å–è§†é¢‘ä¿¡æ¯
        video_info = self._get_video_info(video_id)
        if not video_info:
            print(f"[ERROR] è§†é¢‘ID {video_id} ä¸å­˜åœ¨")
            return None

        action_name = video_info['action_name_en']
        print(f"  åŠ¨ä½œ: {action_name} ({video_info['action_name_cn']})")

        # 2. æ£€æŸ¥æ–‡ä»¶
        if not os.path.exists(video_info['file_path']):
            print(f"  [ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {video_info['file_path']}")
            return None

        # 3. æå–landmarksåºåˆ—å’Œframes
        landmarks_seq, frames_seq = self._extract_sequence(
            video_info['file_path'],
            video_info['start_frame'],
            video_info['end_frame']
        )

        if not landmarks_seq:
            print(f"  [ERROR] å…³é”®ç‚¹æå–å¤±è´¥")
            return None

        # 4. èŽ·å–åŠ¨ä½œæ£€æµ‹å™¨
        detector = self.action_detectors.get(action_name)
        if not detector:
            print(f"  [ERROR] æœªæ‰¾åˆ°åŠ¨ä½œæ£€æµ‹å™¨: {action_name}")
            # ðŸ”§ ä¿®å¤: é‡Šæ”¾å·²æå–çš„åºåˆ—
            del landmarks_seq
            del frames_seq
            return None

        # 5. ä½¿ç”¨åŠ¨ä½œç±»çš„processæ–¹æ³•
        neutral_raw = self._denormalize_indicators(
            neutral_indicators,
            video_info
        ) if neutral_indicators else None

        h, w = frames_seq[0].shape[:2]

        result = detector.process(
            landmarks_seq=landmarks_seq,
            frames_seq=frames_seq,
            w=w,
            h=h,
            fps=video_info.get('fps'),
            neutral_indicators=neutral_raw
        )

        # ðŸ”§ å…³é”®ä¿®å¤5: ç«‹å³é‡Šæ”¾åºåˆ—
        del landmarks_seq
        del frames_seq

        if not result:
            print(f"  [ERROR] å¤„ç†å¤±è´¥")
            return None

        # 6. æå–ç‰¹å¾å‘é‡
        feature_vector = self.feature_integrator.extract_action_features(
            action_name,
            result['normalized_indicators'],
            result['normalized_dynamic_features']
        )

        print(f"  âœ“ ç‰¹å¾ç»´åº¦: {feature_vector.shape[0]}")

        # 7. ä¿å­˜å³°å€¼å¸§
        peak_frame_path = self._save_peak_frame(
            result['peak_frame'],
            video_info['examination_id'],
            action_name
        )

        # 8. å­˜å‚¨åˆ°æ•°æ®åº“
        self._save_to_database(
            video_id=video_id,
            peak_frame_idx=result['peak_frame_idx'],
            peak_frame_path=str(peak_frame_path),
            unit_length=result['unit_length'],
            feature_vector=feature_vector,
            normalized_indicators=result['normalized_indicators'],
            normalized_dynamic_features=result['normalized_dynamic_features']
        )

        return {
            'video_id': video_id,
            'action_name': action_name,
            'peak_frame_idx': result['peak_frame_idx'],
            'peak_frame_path': str(peak_frame_path),
            'unit_length': result['unit_length'],
            'feature_dim': feature_vector.shape[0],
            'feature_vector': feature_vector,
            'normalized_indicators': result['normalized_indicators'],
            'normalized_dynamic_features': result['normalized_dynamic_features']
        }

    def process_all_examinations(self, batch_size=10):
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„examinations

        Args:
            batch_size: æ¯æ‰¹å¤„ç†å¤šå°‘ä¸ªexaminationåŽæ¸…ç†å†…å­˜
        """
        # èŽ·å–æ‰€æœ‰æœªå®Œå…¨å¤„ç†çš„examinations
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT DISTINCT e.examination_id
            FROM examinations e
            INNER JOIN video_files vf ON e.examination_id = vf.examination_id
            LEFT JOIN video_features feat ON vf.video_id = feat.video_id
            WHERE vf.file_exists = 1 AND feat.feature_id IS NULL
            ORDER BY e.examination_id
        """)

        examination_ids = [row[0] for row in cursor.fetchall()]
        conn.close()

        print(f"\næ‰¾åˆ° {len(examination_ids)} ä¸ªéœ€è¦å¤„ç†çš„æ£€æŸ¥")
        print(f"å°†åˆ† {(len(examination_ids) + batch_size - 1) // batch_size} æ‰¹å¤„ç†")

        results = []
        for i, exam_id in enumerate(examination_ids, 1):
            print(f"\n{'#' * 60}")
            print(f"è¿›åº¦: {i}/{len(examination_ids)}")
            print(f"{'#' * 60}")

            try:
                result = self.process_examination(exam_id)
                if result:
                    results.append(result)

                # ðŸ”§ å…³é”®ä¿®å¤6: å®šæœŸæ¸…ç†å†…å­˜
                if i % batch_size == 0:
                    gc.collect()
                    print(f"\n  [å†…å­˜æ¸…ç†] å·²å¤„ç† {i}/{len(examination_ids)} ä¸ªæ£€æŸ¥")

            except Exception as e:
                print(f"[ERROR] å¤„ç†æ£€æŸ¥ {exam_id} æ—¶å‡ºé”™: {str(e)}")
                import traceback
                traceback.print_exc()
                # å‡ºé”™åŽä¹Ÿè¦æ¸…ç†
                gc.collect()

        print(f"\n{'=' * 60}")
        print(f"æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"æˆåŠŸ: {len(results)}/{len(examination_ids)}")
        print(f"{'=' * 60}")

        return results

    def _get_video_info(self, video_id):
        """ä»Žæ•°æ®åº“èŽ·å–è§†é¢‘ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("""
            SELECT 
                vf.video_id,
                vf.examination_id,
                at.action_name_en,
                at.action_name_cn,
                vf.file_path,
                vf.start_frame,
                vf.end_frame,
                vf.fps
            FROM video_files vf
            INNER JOIN action_types at ON vf.action_id = at.action_id
            WHERE vf.video_id = ?
        """, (video_id,))

        row = cursor.fetchone()
        conn.close()

        return dict(row) if row else None

    def _get_examination_videos(self, examination_id):
        """èŽ·å–æŸä¸ªexaminationçš„æ‰€æœ‰è§†é¢‘"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("""
            SELECT 
                vf.video_id,
                vf.examination_id,
                at.action_name_en,
                at.action_name_cn,
                vf.file_path,
                vf.start_frame,
                vf.end_frame,
                vf.fps
            FROM video_files vf
            INNER JOIN action_types at ON vf.action_id = at.action_id
            WHERE vf.examination_id = ?
            AND vf.file_exists = 1
            ORDER BY at.display_order
        """, (examination_id,))

        rows = cursor.fetchall()
        conn.close()

        return [dict(row) for row in rows]

    def _extract_sequence(self, video_path, start_frame, end_frame, extractor=None):
        """
        æå–è§†é¢‘åºåˆ—

        ðŸ”§ å…³é”®ä¿®å¤7: ä½¿ç”¨copy()å¹¶åŠæ—¶é‡Šæ”¾åŽŸå§‹å¸§
        """
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            return None, None

        extractor = extractor or self.landmark_extractor

        landmarks_seq = []
        frames_seq = []

        if start_frame > 0:
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        frame_idx = start_frame

        while cap.isOpened() and frame_idx <= end_frame:
            ret, frame = cap.read()
            if not ret:
                break

            # MediaPipeæå–
            landmarks = extractor.extract_from_frame(frame)

            landmarks_seq.append(landmarks)
            # ðŸ”§ å…³é”®: åªä¿ç•™å‰¯æœ¬,åŽŸå¸§ç«‹å³é‡Šæ”¾
            frames_seq.append(frame.copy())
            del frame

            frame_idx += 1

        cap.release()

        return landmarks_seq, frames_seq

    def _compute_video_only(self, video_info, neutral_indicators=None):
        """
        å·¥ä½œçº¿ç¨‹ä¸­è®¡ç®—å•ä¸ªè§†é¢‘

        ðŸ”§ å…³é”®ä¿®å¤8: å¤„ç†å®Œç«‹å³é‡Šæ”¾åºåˆ—
        """
        t0 = time.perf_counter()
        action_name = video_info['action_name_en']

        if not os.path.exists(video_info['file_path']):
            return {"ok": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {video_info['file_path']}"}

        worker = self._get_worker()

        landmarks_seq, frames_seq = self._extract_sequence(
            video_info['file_path'],
            video_info['start_frame'],
            video_info['end_frame'],
            extractor=worker.landmark_extractor
        )
        if not landmarks_seq:
            return {"ok": False, "error": "å…³é”®ç‚¹æå–å¤±è´¥"}

        detector = worker.action_detectors.get(action_name)
        if not detector:
            return {"ok": False, "error": f"æœªæ‰¾åˆ°åŠ¨ä½œæ£€æµ‹å™¨: {action_name}"}

        neutral_raw = self._denormalize_indicators(neutral_indicators, video_info) if neutral_indicators else None

        h, w = frames_seq[0].shape[:2]
        result = detector.process(
            landmarks_seq=landmarks_seq,
            frames_seq=frames_seq,
            w=w,
            h=h,
            fps=video_info.get('fps'),
            neutral_indicators=neutral_raw
        )

        # ðŸ”§ å…³é”®: ç«‹å³é‡Šæ”¾åºåˆ—å†…å­˜
        del landmarks_seq
        del frames_seq

        if not result:
            return {"ok": False, "error": "åŠ¨ä½œå¤„ç†å¤±è´¥(detector.process è¿”å›žç©º)"}

        feature_vector = worker.feature_integrator.extract_action_features(
            action_name,
            result['normalized_indicators'],
            result['normalized_dynamic_features']
        )

        return {
            "ok": True,
            "action_name": action_name,
            "video_id": video_info["video_id"],
            "examination_id": video_info["examination_id"],
            "result": result,
            "feature_vector": feature_vector,
            "elapsed_ms": (time.perf_counter() - t0) * 1000.0
        }

    def _denormalize_indicators(self, normalized_indicators, video_info):
        """å°†å½’ä¸€åŒ–æŒ‡æ ‡è½¬æ¢å›žåŽŸå§‹åƒç´ å€¼(ç”¨äºŽåŠ¨ä½œç±»)"""
        return normalized_indicators

    def _save_peak_frame(self, frame, examination_id, action_name):
        """ä¿å­˜å³°å€¼å¸§"""
        action_dir = self.keyframe_root_dir / action_name
        action_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{examination_id}_{action_name}.jpg"
        filepath = action_dir / filename

        cv2.imwrite(str(filepath), frame)

        return filepath

    def _save_to_database(self, video_id, peak_frame_idx, peak_frame_path,
                          unit_length, feature_vector, normalized_indicators,
                          normalized_dynamic_features):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        video_info = self._get_video_info(video_id)
        action_name = video_info['action_name_en']

        key_indicators = self.feature_integrator.action_key_indicators.get(action_name, None)
        if key_indicators is None:
            print(f"  [WARN] æœªåœ¨ action_key_indicators ä¸­æ‰¾åˆ° {action_name}ï¼Œè·³è¿‡å…¥åº“")
            return

        static_names = key_indicators['static']
        dynamic_names = key_indicators['dynamic']

        static_vals = [
            float(normalized_indicators.get(name, 0.0))
            for name in static_names
        ]
        dynamic_vals = [
            float(normalized_dynamic_features.get(name, 0.0))
            for name in dynamic_names
        ]

        static_arr = np.array(static_vals, dtype=np.float32)
        dynamic_arr = np.array(dynamic_vals, dtype=np.float32)

        static_blob = static_arr.tobytes()
        dynamic_blob = dynamic_arr.tobytes()

        static_dim = len(static_vals)
        dynamic_dim = len(dynamic_vals)

        try:
            cursor.execute("""
                INSERT INTO video_features (
                    video_id,
                    peak_frame_idx,
                    peak_frame_path,
                    unit_length,
                    static_features,
                    dynamic_features,
                    static_dim,
                    dynamic_dim
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                video_id,
                peak_frame_idx,
                peak_frame_path,
                unit_length,
                static_blob,
                dynamic_blob,
                static_dim,
                dynamic_dim
            ))

            conn.commit()

        except sqlite3.IntegrityError:
            # æ›´æ–°
            cursor.execute("""
                UPDATE video_features
                SET peak_frame_idx = ?,
                    peak_frame_path = ?,
                    unit_length = ?,
                    static_features = ?,
                    dynamic_features = ?,
                    static_dim = ?,
                    dynamic_dim = ?,
                    geometry_processed_at = CURRENT_TIMESTAMP
                WHERE video_id = ?
            """, (
                peak_frame_idx,
                peak_frame_path,
                unit_length,
                static_blob,
                dynamic_blob,
                static_dim,
                dynamic_dim,
                video_id
            ))

            conn.commit()

        finally:
            conn.close()


def main():
    """ä¸»å‡½æ•°ï¼šæ–¹ä¾¿åœ¨ PyCharm é‡Œä¸€é”®è¿è¡Œ"""

    # åŸºæœ¬è·¯å¾„é…ç½®
    db_path = 'facialPalsy.db'
    model_path = '/Users/cuijinglei/PycharmProjects/medicalProject/models/face_landmarker.task'
    keyframe_dir = '/Users/cuijinglei/Documents/facialPalsy/HGFA/keyframes'
    os.makedirs(keyframe_dir, exist_ok=True)

    examination_id = None
    video_id = None
    run_batch = True

    # åˆå§‹åŒ– Pipeline
    pipeline = VideoPipeline(db_path, model_path, keyframe_dir)

    if examination_id is not None:
        pipeline.process_examination(examination_id)
    elif video_id is not None:
        pipeline.process_video(video_id)
    elif run_batch:
        # ðŸ”§ å…³é”®: ä½¿ç”¨åˆ†æ‰¹å¤„ç†,æ¯10ä¸ªexaminationæ¸…ç†ä¸€æ¬¡
        pipeline.process_all_examinations(batch_size=10)
    else:
        print("å½“å‰æ²¡æœ‰é…ç½®ä»»ä½•å¤„ç†ä»»åŠ¡")


if __name__ == '__main__':
    main()