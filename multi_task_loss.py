"""
å¤šä»»åŠ¡å­¦ä¹ æ¨¡å— (Multi-Task Learning)
====================================

å®ç°ç­–ç•¥: Uncertainty Weighting (ä¸ç¡®å®šæ€§åŠ æƒ)

åŸç†:
- æ¯ä¸ªä»»åŠ¡æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„logæ–¹å·®å‚æ•° (log_sigma)
- ä»»åŠ¡æŸå¤± = loss / (2 * sigma^2) + log(sigma)
- è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„æƒé‡

å‚è€ƒæ–‡çŒ®:
- Kendall et al., "Multi-Task Learning Using Uncertainty to Weigh Losses
  for Scene Geometry and Semantics", CVPR 2018

ä»»åŠ¡åˆ—è¡¨:
1. åŠ¨ä½œçº§ä»»åŠ¡:
   - severity_classification: ä¸¥é‡ç¨‹åº¦åˆ†ç±» (1-5)

2. æ£€æŸ¥çº§ä»»åŠ¡:
   - palsy_detection: é¢ç˜«æ£€æµ‹ (äºŒåˆ†ç±»)
   - side_classification: æ‚£ä¾§è¯†åˆ« (ä¸‰åˆ†ç±»: æ— /å·¦/å³)
   - hb_grading: HBåˆ†çº§ (1-6)
   - sunnybrook_regression: Sunnybrookè¯„åˆ† (å›å½’)

ç”¨æ³•:
    from multi_task_loss import MultiTaskLoss

    loss_fn = MultiTaskLoss(tasks=['severity', 'hb', 'sunnybrook'])
    total_loss, task_losses = loss_fn(predictions, targets)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass


# =============================================================================
# ä»»åŠ¡é…ç½®
# =============================================================================

@dataclass
class TaskConfig:
    """ä»»åŠ¡é…ç½®"""
    name: str
    task_type: str  # 'classification', 'regression', 'ordinal'
    num_classes: int = 0  # åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°
    loss_fn: str = 'ce'  # 'ce', 'bce', 'mse', 'l1', 'ordinal'
    weight: float = 1.0  # åˆå§‹æƒé‡
    level: str = 'action'  # 'action' or 'examination'


# é¢„å®šä¹‰ä»»åŠ¡é…ç½®
TASK_CONFIGS = {
    # åŠ¨ä½œçº§ä»»åŠ¡
    'severity': TaskConfig(
        name='severity',
        task_type='classification',
        num_classes=5,
        loss_fn='ce',
        level='action'
    ),

    # æ£€æŸ¥çº§ä»»åŠ¡
    'palsy_detection': TaskConfig(
        name='palsy_detection',
        task_type='classification',
        num_classes=2,
        loss_fn='bce',
        level='examination'
    ),

    'side_classification': TaskConfig(
        name='side_classification',
        task_type='classification',
        num_classes=3,  # æ— /å·¦/å³
        loss_fn='ce',
        level='examination'
    ),

    'hb_grading': TaskConfig(
        name='hb_grading',
        task_type='ordinal',  # æœ‰åºåˆ†ç±»
        num_classes=6,
        loss_fn='ordinal',
        level='examination'
    ),

    'sunnybrook': TaskConfig(
        name='sunnybrook',
        task_type='regression',
        loss_fn='mse',
        level='examination'
    ),
}


# =============================================================================
# Ordinal Loss (CORN Loss for ordinal regression)
# =============================================================================

class OrdinalLoss(nn.Module):
    """
    æœ‰åºåˆ†ç±»æŸå¤± (ç”¨äºHBåˆ†çº§ç­‰æœ‰åºæ ‡ç­¾)

    ä½¿ç”¨ç´¯ç§¯æ¦‚ç‡æ–¹æ³•: P(Y > k) for k = 0, 1, ..., K-1
    """

    def __init__(self, num_classes: int):
        super().__init__()
        self.num_classes = num_classes

    def forward(
            self,
            logits: torch.Tensor,  # (B, num_classes)
            targets: torch.Tensor  # (B,) 0-indexed
    ) -> torch.Tensor:
        """
        Args:
            logits: æ¨¡å‹è¾“å‡º
            targets: ç›®æ ‡ç±»åˆ« (0 to K-1)

        Returns:
            loss: æ ‡é‡æŸå¤±
        """
        batch_size = logits.size(0)
        num_classes = logits.size(1)

        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        probs = F.softmax(logits, dim=1)
        cum_probs = torch.cumsum(probs, dim=1)  # P(Y <= k)

        # æ„é€ ç›®æ ‡: å¯¹äºç±»åˆ«cï¼Œy_k = 1 if k < c, else 0
        targets_expanded = targets.unsqueeze(1).expand(-1, num_classes - 1)
        threshold = torch.arange(1, num_classes, device=logits.device).unsqueeze(0)
        binary_targets = (targets_expanded >= threshold).float()  # (B, K-1)

        # äºŒå…ƒäº¤å‰ç†µ
        cum_probs_clipped = torch.clamp(cum_probs[:, :-1], 1e-7, 1 - 1e-7)

        loss = F.binary_cross_entropy(
            cum_probs_clipped,
            binary_targets,
            reduction='mean'
        )

        return loss


# =============================================================================
# Label Smoothing Loss
# =============================================================================

class LabelSmoothingCrossEntropy(nn.Module):
    """æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±"""

    def __init__(self, smoothing: float = 0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(
            self,
            logits: torch.Tensor,
            targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (B, C)
            targets: (B,) ç±»åˆ«ç´¢å¼•
        """
        num_classes = logits.size(1)

        # æ„é€ å¹³æ»‘æ ‡ç­¾
        with torch.no_grad():
            smooth_targets = torch.full_like(
                logits,
                self.smoothing / (num_classes - 1)
            )
            smooth_targets.scatter_(
                1,
                targets.unsqueeze(1),
                1.0 - self.smoothing
            )

        # KLæ•£åº¦
        log_probs = F.log_softmax(logits, dim=1)
        loss = -(smooth_targets * log_probs).sum(dim=1).mean()

        return loss


# =============================================================================
# Focal Loss (å¤„ç†ç±»åˆ«ä¸å¹³è¡¡)
# =============================================================================

class FocalLoss(nn.Module):
    """Focal Lossç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""

    def __init__(
            self,
            gamma: float = 2.0,
            alpha: Optional[torch.Tensor] = None
    ):
        """
        Args:
            gamma: focusing parameter
            alpha: ç±»åˆ«æƒé‡ (å¯é€‰)
        """
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(
            self,
            logits: torch.Tensor,
            targets: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            logits: (B, C)
            targets: (B,)
        """
        ce_loss = F.cross_entropy(logits, targets, reduction='none')

        probs = F.softmax(logits, dim=1)
        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)

        focal_weight = (1 - pt) ** self.gamma

        if self.alpha is not None:
            alpha_weight = self.alpha.to(logits.device).gather(0, targets)
            focal_weight = focal_weight * alpha_weight

        loss = (focal_weight * ce_loss).mean()

        return loss


# =============================================================================
# Uncertainty Weighting Multi-Task Loss
# =============================================================================

class MultiTaskLoss(nn.Module):
    """
    å¤šä»»åŠ¡å­¦ä¹ æŸå¤± - Uncertainty Weighting

    è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„æƒé‡
    """

    def __init__(
            self,
            task_names: List[str],
            use_focal: bool = True,
            label_smoothing: float = 0.1,
            device: str = 'cpu'
    ):
        """
        åˆå§‹åŒ–

        Args:
            task_names: è¦ä½¿ç”¨çš„ä»»åŠ¡åç§°åˆ—è¡¨
            use_focal: æ˜¯å¦ä½¿ç”¨Focal Loss
            label_smoothing: æ ‡ç­¾å¹³æ»‘ç³»æ•°
            device: è®¾å¤‡
        """
        super().__init__()

        self.task_names = task_names
        self.device = device

        # è·å–ä»»åŠ¡é…ç½®
        self.task_configs = {
            name: TASK_CONFIGS[name]
            for name in task_names
            if name in TASK_CONFIGS
        }

        # å¯å­¦ä¹ çš„logæ–¹å·®å‚æ•° (Uncertainty Weightingæ ¸å¿ƒ)
        self.log_vars = nn.ParameterDict({
            name: nn.Parameter(torch.zeros(1, device=device))
            for name in self.task_configs.keys()
        })

        # åˆå§‹åŒ–å„ä»»åŠ¡çš„æŸå¤±å‡½æ•°
        self.loss_fns = nn.ModuleDict()

        for name, config in self.task_configs.items():
            if config.loss_fn == 'ce':
                if use_focal:
                    self.loss_fns[name] = FocalLoss(gamma=2.0)
                elif label_smoothing > 0:
                    self.loss_fns[name] = LabelSmoothingCrossEntropy(label_smoothing)
                else:
                    self.loss_fns[name] = nn.CrossEntropyLoss()

            elif config.loss_fn == 'bce':
                self.loss_fns[name] = nn.BCEWithLogitsLoss()

            elif config.loss_fn == 'mse':
                self.loss_fns[name] = nn.MSELoss()

            elif config.loss_fn == 'l1':
                self.loss_fns[name] = nn.L1Loss()

            elif config.loss_fn == 'ordinal':
                self.loss_fns[name] = OrdinalLoss(config.num_classes)

        print(f"[MultiTaskLoss] åˆå§‹åŒ– {len(self.task_configs)} ä¸ªä»»åŠ¡:")
        for name in self.task_configs:
            print(f"  - {name}")

    def forward(
            self,
            predictions: Dict[str, torch.Tensor],
            targets: Dict[str, torch.Tensor],
            valid_mask: Optional[Dict[str, torch.Tensor]] = None
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—å¤šä»»åŠ¡åŠ æƒæŸå¤±

        Args:
            predictions: {task_name: prediction_tensor}
            targets: {task_name: target_tensor}
            valid_mask: {task_name: bool_mask} æ ‡è®°æœ‰æ•ˆæ ·æœ¬

        Returns:
            total_loss: åŠ æƒæ€»æŸå¤±
            task_losses: {task_name: task_loss}
        """
        task_losses = {}
        weighted_losses = []

        for name in self.task_configs:
            if name not in predictions or name not in targets:
                continue

            pred = predictions[name]
            target = targets[name]

            # åº”ç”¨æœ‰æ•ˆæ©ç 
            if valid_mask is not None and name in valid_mask:
                mask = valid_mask[name]
                if mask.sum() == 0:
                    continue
                pred = pred[mask]
                target = target[mask]

            # è®¡ç®—åŸå§‹æŸå¤±
            loss_fn = self.loss_fns[name]
            raw_loss = loss_fn(pred, target)

            # Uncertainty Weighting
            # loss = loss / (2 * sigma^2) + log(sigma)
            # = loss * exp(-2*log_var) + log_var
            log_var = self.log_vars[name]

            # ğŸ”§ é™åˆ¶ log_var èŒƒå›´,é˜²æ­¢ exp çˆ†ç‚¸
            log_var = torch.clamp(log_var, min=-10, max=10)

            # è®¡ç®—åŠ æƒæŸå¤±
            weighted_loss = raw_loss * torch.exp(-2 * log_var) + log_var

            # ğŸ”§ æ£€æŸ¥åŠ æƒæŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                # å¦‚æœæ— æ•ˆ,ç›´æ¥ä½¿ç”¨åŸå§‹æŸå¤±
                weighted_loss = raw_loss

            task_losses[name] = raw_loss.detach()
            weighted_losses.append(weighted_loss)

        # æ€»æŸå¤±
        if weighted_losses:
            total_loss = torch.stack(weighted_losses).sum()
        else:
            total_loss = torch.tensor(0.0, device=self.device)

        return total_loss, task_losses

    def get_task_weights(self) -> Dict[str, float]:
        """
        è·å–å½“å‰ä»»åŠ¡æƒé‡ (ç”¨äºç›‘æ§å’Œå¯è§†åŒ–)

        æƒé‡ = 1 / (2 * sigma^2) = exp(-2 * log_var)
        """
        weights = {}
        for name in self.task_configs:
            log_var = self.log_vars[name].item()
            weight = float(torch.exp(-2 * torch.tensor(log_var)))
            weights[name] = weight

        # å½’ä¸€åŒ–
        total = sum(weights.values())
        if total > 0:
            weights = {k: v / total for k, v in weights.items()}

        return weights

    def get_log_vars(self) -> Dict[str, float]:
        """è·å–logæ–¹å·®å‚æ•°"""
        return {
            name: self.log_vars[name].item()
            for name in self.task_configs
        }


# =============================================================================
# å¤šä»»åŠ¡åˆ†ç±»å¤´
# =============================================================================

class MultiTaskHead(nn.Module):
    """
    å¤šä»»åŠ¡åˆ†ç±»å¤´

    ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„åˆ†ç±»/å›å½’å¤´
    """

    def __init__(
            self,
            input_dim: int,
            task_configs: Dict[str, TaskConfig],
            hidden_dim: int = 256,
            dropout: float = 0.3
    ):
        """
        åˆå§‹åŒ–

        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            task_configs: ä»»åŠ¡é…ç½®å­—å…¸
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutç‡
        """
        super().__init__()

        self.task_configs = task_configs

        # å…±äº«çš„ç‰¹å¾å˜æ¢å±‚
        self.shared_mlp = nn.Sequential(
            nn.LayerNorm(input_dim),
            nn.Linear(input_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )

        # æ¯ä¸ªä»»åŠ¡çš„ä¸“å±å¤´
        self.task_heads = nn.ModuleDict()

        for name, config in task_configs.items():
            if config.task_type == 'regression':
                # å›å½’ä»»åŠ¡
                self.task_heads[name] = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_dim // 2, 1)
                )
            else:
                # åˆ†ç±»ä»»åŠ¡
                self.task_heads[name] = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_dim // 2, config.num_classes)
                )

    def forward(
            self,
            features: torch.Tensor,
            task_names: Optional[List[str]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            features: (B, input_dim) è¾“å…¥ç‰¹å¾
            task_names: è¦è®¡ç®—çš„ä»»åŠ¡åˆ—è¡¨ (Noneè¡¨ç¤ºå…¨éƒ¨)

        Returns:
            outputs: {task_name: prediction}
        """
        # å…±äº«å˜æ¢
        shared_features = self.shared_mlp(features)

        # å„ä»»åŠ¡é¢„æµ‹
        outputs = {}

        for name, head in self.task_heads.items():
            if task_names is not None and name not in task_names:
                continue

            outputs[name] = head(shared_features)

            # å›å½’ä»»åŠ¡è¾“å‡ºsqueeze
            if self.task_configs[name].task_type == 'regression':
                outputs[name] = outputs[name].squeeze(-1)

        return outputs


# =============================================================================
# æ£€æŸ¥çº§ç‰¹å¾èšåˆ
# =============================================================================

class ExaminationAggregator(nn.Module):
    """
    æ£€æŸ¥çº§ç‰¹å¾èšåˆå™¨

    å°†åŒä¸€æ£€æŸ¥çš„å¤šä¸ªåŠ¨ä½œç‰¹å¾èšåˆä¸ºä¸€ä¸ªæ£€æŸ¥çº§ç‰¹å¾
    """

    def __init__(
            self,
            input_dim: int,
            num_actions: int = 11,
            aggregation: str = 'attention',
            hidden_dim: int = 256
    ):
        """
        Args:
            input_dim: æ¯ä¸ªåŠ¨ä½œçš„ç‰¹å¾ç»´åº¦
            num_actions: åŠ¨ä½œæ•°é‡
            aggregation: èšåˆæ–¹å¼ ('mean', 'max', 'attention')
            hidden_dim: æ³¨æ„åŠ›éšè—ç»´åº¦
        """
        super().__init__()

        self.input_dim = input_dim
        self.num_actions = num_actions
        self.aggregation = aggregation

        if aggregation == 'attention':
            self.attention = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, 1)
            )

    def forward(
            self,
            action_features: torch.Tensor,
            action_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            action_features: (B, num_actions, input_dim)
            action_mask: (B, num_actions) æœ‰æ•ˆåŠ¨ä½œæ©ç 

        Returns:
            exam_features: (B, input_dim)
        """
        if self.aggregation == 'mean':
            if action_mask is not None:
                # æ©ç å¹³å‡
                mask = action_mask.unsqueeze(-1).float()
                summed = (action_features * mask).sum(dim=1)
                counts = mask.sum(dim=1).clamp(min=1)
                return summed / counts
            else:
                return action_features.mean(dim=1)

        elif self.aggregation == 'max':
            if action_mask is not None:
                action_features = action_features.masked_fill(
                    ~action_mask.unsqueeze(-1),
                    float('-inf')
                )
            return action_features.max(dim=1)[0]

        elif self.aggregation == 'attention':
            # æ³¨æ„åŠ›åŠ æƒ
            scores = self.attention(action_features).squeeze(-1)  # (B, num_actions)

            if action_mask is not None:
                scores = scores.masked_fill(~action_mask, float('-inf'))

            weights = F.softmax(scores, dim=1).unsqueeze(-1)  # (B, num_actions, 1)
            return (action_features * weights).sum(dim=1)  # (B, input_dim)

        else:
            raise ValueError(f"Unknown aggregation: {self.aggregation}")


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def create_multi_task_loss(
        task_names: List[str],
        device: str = 'cpu'
) -> MultiTaskLoss:
    """åˆ›å»ºå¤šä»»åŠ¡æŸå¤±å‡½æ•°"""
    return MultiTaskLoss(
        task_names=task_names,
        use_focal=True,
        label_smoothing=0.1,
        device=device
    )


def print_task_weights(loss_fn: MultiTaskLoss):
    """æ‰“å°å½“å‰ä»»åŠ¡æƒé‡"""
    weights = loss_fn.get_task_weights()
    log_vars = loss_fn.get_log_vars()

    print("\nä»»åŠ¡æƒé‡:")
    for name in sorted(weights.keys()):
        print(f"  {name:20} weight={weights[name]:.4f}  log_var={log_vars[name]:.4f}")


# =============================================================================
# æµ‹è¯•
# =============================================================================

if __name__ == '__main__':
    # æµ‹è¯•å¤šä»»åŠ¡æŸå¤±
    print("æµ‹è¯• MultiTaskLoss...")

    loss_fn = MultiTaskLoss(
        task_names=['severity', 'hb_grading', 'sunnybrook'],
        use_focal=True,
        device='mps'
    )

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 8

    predictions = {
        'severity': torch.randn(batch_size, 5),
        'hb_grading': torch.randn(batch_size, 6),
        'sunnybrook': torch.randn(batch_size),
    }

    targets = {
        'severity': torch.randint(0, 5, (batch_size,)),
        'hb_grading': torch.randint(0, 6, (batch_size,)),
        'sunnybrook': torch.rand(batch_size) * 100,
    }

    # è®¡ç®—æŸå¤±
    total_loss, task_losses = loss_fn(predictions, targets)

    print(f"\næ€»æŸå¤±: {total_loss.item():.4f}")
    print("å„ä»»åŠ¡æŸå¤±:")
    for name, loss in task_losses.items():
        print(f"  {name}: {loss.item():.4f}")

    print_task_weights(loss_fn)

    # æµ‹è¯•å¤šä»»åŠ¡å¤´
    print("\næµ‹è¯• MultiTaskHead...")

    task_configs = {
        'severity': TASK_CONFIGS['severity'],
        'hb_grading': TASK_CONFIGS['hb_grading'],
    }

    head = MultiTaskHead(
        input_dim=512,
        task_configs=task_configs
    )

    features = torch.randn(batch_size, 512)
    outputs = head(features)

    for name, output in outputs.items():
        print(f"  {name}: {output.shape}")

    print("\nâœ“ æµ‹è¯•é€šè¿‡!")